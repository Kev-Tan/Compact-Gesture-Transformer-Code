{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa8336f",
   "metadata": {},
   "source": [
    "### Hyperparameters and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "a0d16ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything that is necessary\n",
    "import argparse\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from torch.utils.data import DataLoader\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "from timm import create_model\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import MultiStepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "5464bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    # Network params\n",
    "    'n_classes': 12,\n",
    "    'pretrained': True,\n",
    "    'n_head': 8,\n",
    "    'dropout_backbone': 0.1,  # dropout2d\n",
    "    'dropout_transformer': 0.5,  # dropout1d\n",
    "    'dff': 1024,  # ff_size\n",
    "    'n_module': 6,\n",
    "    \n",
    "    # Solver params (required by ModuleUtilizer)\n",
    "    'solver': {\n",
    "        'type': 'AdamW',  # or 'Adam', 'SGD', 'RMSProp'\n",
    "        'base_lr': 0.0001,\n",
    "        'weight_decay': 0.0001,\n",
    "        'momentum': 0.9,  # only used for SGD\n",
    "        'lr_policy': 'fixed',  # or 'step', 'multistep', 'exp', 'inv'\n",
    "        'gamma': 0.1,\n",
    "        'stepvalue': [50, 75]  # for multistep policy\n",
    "    },\n",
    "    \n",
    "    # Checkpoint params (required by ModuleUtilizer)\n",
    "    'checkpoints': {\n",
    "        'save_policy': 'best',  # or 'all', 'early_stop'\n",
    "        'save_name': 'gesture_model',\n",
    "        'save_dir': './checkpoints/',\n",
    "        'early_stop': 10  # patience for early stopping\n",
    "    },\n",
    "    \n",
    "    # Other required params\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'dataset': 'Briareo',\n",
    "    'gpu': [0],  # GPU IDs for DataParallel\n",
    "    'resume': None  # Path to checkpoint to resume from, or None\n",
    "}\n",
    "\n",
    "# Decide if we use timm_backbone or not\n",
    "timm_backbone = True\n",
    "model_name = \"resnet18.a1_in1k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "9e617de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch;\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f19545f",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "40eaa033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Briareo(Dataset):\n",
    "    \"\"\"Briareo Dataset class\"\"\"\n",
    "    def __init__(self, configer, path, split=\"train\", data_type='depth', transforms=None, n_frames=30, optical_flow=False):\n",
    "        \"\"\"Constructor method for Briareo Dataset class\n",
    "\n",
    "        Args:\n",
    "            configer (Configer): Configer object for current procedure phase (train, test, val)\n",
    "            split (str, optional): Current procedure phase (train, test, val)\n",
    "            data_type (str, optional): Input data type (depth, rgb, normals, ir)\n",
    "            transform (Object, optional): Data augmentation transformation for every data\n",
    "            n_frames (int, optional): Number of frames selected for every input clip\n",
    "            optical_flow (bool, optional): Flag to choose if calculate optical flow or not\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset_path = Path(path)\n",
    "        self.split = split\n",
    "        self.data_type = data_type\n",
    "        self.optical_flow = optical_flow\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.n_frames = n_frames + 1\n",
    "\n",
    "        print(\"Loading Briareo {} dataset...\".format(split.upper()), end=\" \")\n",
    "        data = np.load(self.dataset_path / \"splits\" / (self.split if self.split != \"val\" else \"train\") /\n",
    "                                    \"{}_{}.npz\".format(data_type, self.split), allow_pickle=True)['arr_0']\n",
    "\n",
    "        # Prepare clip for the selected number of frames n_frame\n",
    "        fixed_data = list()\n",
    "        for i, record in enumerate(data):\n",
    "            paths = record['data']\n",
    "\n",
    "            center_of_list = math.floor(len(paths) / 2)\n",
    "            crop_limit = math.floor(self.n_frames / 2)\n",
    "\n",
    "            start = center_of_list - crop_limit\n",
    "            end = center_of_list + crop_limit\n",
    "            paths_cropped = paths[start: end + 1 if self.n_frames % 2 == 1 else end]\n",
    "            if self.data_type == 'leapmotion':\n",
    "                valid = np.array(record['valid'][start: end + 1 if self.n_frames % 2 == 1 else end])\n",
    "                if valid.sum() == len(valid):\n",
    "                    data[i]['data'] = paths_cropped\n",
    "                    fixed_data.append(data[i])\n",
    "            else:\n",
    "                data[i]['data'] = paths_cropped\n",
    "                fixed_data.append(data[i])\n",
    "\n",
    "        self.data = np.array(fixed_data)\n",
    "        print(\"done.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        paths = self.data[idx]['data']\n",
    "        label = self.data[idx]['label']\n",
    "\n",
    "        clip = list()\n",
    "        for p in paths:\n",
    "            img = cv2.imread(str(self.dataset_path / p), cv2.IMREAD_COLOR)\n",
    "            img = cv2.resize(img, (224, 224))\n",
    "            if self.data_type != \"rgb\":\n",
    "                img = np.expand_dims(img, axis=2)\n",
    "            clip.append(img)\n",
    "\n",
    "        clip = np.array(clip).transpose(1, 2, 3, 0)\n",
    "\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            aug_det = self.transforms.to_deterministic()\n",
    "            clip = np.array([aug_det.augment_image(clip[..., i]) for i in range(clip.shape[-1])]).transpose(1, 2, 3, 0)\n",
    "\n",
    "        clip = torch.from_numpy(clip.reshape(clip.shape[0], clip.shape[1], -1).transpose(2, 0, 1))\n",
    "        label = torch.LongTensor(np.asarray([label]))\n",
    "        return clip.float(), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "7d3c4ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader():\n",
    "    dataset_path = \"D:\\School\\Lab\\Compact-Gesture-Transformer-Code\\Briareo_rgb\"\n",
    "    train_loader = DataLoader(Briareo(configer=None, path=dataset_path, data_type=\"rgb\", split=\"train\"))\n",
    "    test_loader = DataLoader(Briareo(configer=None, path=dataset_path, data_type=\"rgb\", split=\"test\"))\n",
    "    val_loader = DataLoader(Briareo(configer=None, path=dataset_path, data_type=\"rgb\", split=\"val\"))\n",
    "    return train_loader, test_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165a80c",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "0e317b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def position_embedding(input, d_model):\n",
    "    input = input.view(-1, 1)\n",
    "    dim = torch.arange(d_model // 2, dtype=torch.float32, device=input.device).view(1, -1)\n",
    "    sin = torch.sin(input / 10000 ** (2 * dim / d_model))\n",
    "    cos = torch.cos(input / 10000 ** (2 * dim / d_model))\n",
    "\n",
    "    out = torch.zeros((input.shape[0], d_model), device=input.device)\n",
    "    out[:, ::2] = sin\n",
    "    out[:, 1::2] = cos\n",
    "    return out\n",
    "\n",
    "def sinusoid_encoding_table(max_len, d_model):\n",
    "    pos = torch.arange(max_len, dtype=torch.float32)\n",
    "    out = position_embedding(pos, d_model)\n",
    "    return out\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_k, d_v, h):\n",
    "        \"\"\"\n",
    "        :param d_model: Output dimensionality of the model\n",
    "        :param d_k: Dimensionality of queries and keys\n",
    "        :param d_v: Dimensionality of values\n",
    "        :param h: Number of heads\n",
    "        \"\"\"\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.fc_q = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_k = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_v = nn.Linear(d_model, h * d_v)\n",
    "        self.fc_o = nn.Linear(h * d_v, d_model)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.h = h\n",
    "\n",
    "        self.init_weights(gain=1.0)\n",
    "\n",
    "    def init_weights(self, gain=1.0):\n",
    "        nn.init.xavier_normal_(self.fc_q.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_k.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_v.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_o.weight, gain=gain)\n",
    "        nn.init.constant_(self.fc_q.bias, 0)\n",
    "        nn.init.constant_(self.fc_k.bias, 0)\n",
    "        nn.init.constant_(self.fc_v.bias, 0)\n",
    "        nn.init.constant_(self.fc_o.bias, 0)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"\n",
    "        Computes\n",
    "        :param queries: Queries (b_s, nq, d_model)\n",
    "        :param keys: Keys (b_s, nk, d_model)\n",
    "        :param values: Values (b_s, nk, d_model)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        b_s, nq = queries.shape[:2]\n",
    "        nk = keys.shape[1]\n",
    "        q = self.fc_q(queries).view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n",
    "        k = self.fc_k(keys).view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n",
    "        v = self.fc_v(values).view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n",
    "\n",
    "        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n",
    "\n",
    "        att = torch.softmax(att, -1)\n",
    "\n",
    "        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n",
    "        out = self.fc_o(out)  # (b_s, nq, d_model)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention layer with Dropout and Layer Normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_k, d_v, h, dff=2048, dropout=.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(d_model=d_model, d_k=d_k, d_v=d_v, h=h)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        # self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Sequential(*[nn.Linear(d_model, dff), nn.ReLU(inplace=True), nn.Dropout(p=dropout),\n",
    "                                  nn.Linear(dff, d_model)])\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        att = self.attention(queries, keys, values)\n",
    "        att = self.dropout(att)\n",
    "        # att = self.layer_norm(queries + att)\n",
    "        att = self.fc(att)\n",
    "        att = self.dropout(att)\n",
    "        return self.layer_norm(queries + att)\n",
    "\n",
    "class EncoderSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, n_head, dff=2048, dropout_transformer=.1, n_module=6):\n",
    "        super(EncoderSelfAttention, self).__init__()\n",
    "        self.encoder = nn.ModuleList([MultiHeadAttention(d_model, d_k, d_v, n_head, dff, dropout_transformer)\n",
    "                                      for _ in range(n_module)])\n",
    "    def forward(self, x):\n",
    "        in_encoder = x + sinusoid_encoding_table(x.shape[1], x.shape[2]).expand(x.shape).to(x.device)\n",
    "        for l in self.encoder:\n",
    "            in_encoder = l(in_encoder, in_encoder, in_encoder)\n",
    "        return in_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bca6e5",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "62662602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_timm_backbone(model_name=model_name, hyperparams=hyperparams):\n",
    "    print('build_model args')\n",
    "    print(f\"Creating model: {model_name}\")\n",
    "        \n",
    "    if 'vit' in model_name or 'deit' in model_name:\n",
    "        model = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=hyperparams.get('pretrained', False),\n",
    "            pretrained_cfg=None,\n",
    "            pretrained_cfg_overlay=None,\n",
    "            num_classes=0,\n",
    "            drop_rate=hyperparams.get('dropout2d', 0.0),\n",
    "            drop_path_rate=hyperparams.get('drop_path', 0.0),\n",
    "            drop_block_rate=None,\n",
    "            img_size=hyperparams.get('input_size', 224)\n",
    "        )\n",
    "    else:\n",
    "        try:\n",
    "            model = timm.create_model(\n",
    "                model_name,\n",
    "                pretrained=hyperparams.get('pretrained', False),\n",
    "                pretrained_cfg=None,\n",
    "                pretrained_cfg_overlay=None,\n",
    "                num_classes=0,\n",
    "                drop_rate=hyperparams.get('dropout2d', 0.0),\n",
    "                drop_path_rate=hyperparams.get('drop_path', 0.0),\n",
    "                drop_block_rate=None\n",
    "            )\n",
    "        except:\n",
    "            model = timm.create_model(\n",
    "                model_name,\n",
    "                pretrained=hyperparams.get('pretrained', False),\n",
    "                pretrained_cfg=None,\n",
    "                pretrained_cfg_overlay=None,\n",
    "                num_classes=0,\n",
    "                drop_rate=hyperparams.get('dropout2d', 0.0),\n",
    "                drop_path_rate=hyperparams.get('drop_path', 0.0),\n",
    "                drop_block_rate=None\n",
    "            )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "1a524262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _GestureTransformer(nn.Module):\n",
    "    \"\"\"Multi Modal model for gesture recognition on 3 channel\"\"\"\n",
    "    def __init__(self, backbone: nn.Module, in_planes: int, out_planes: int,\n",
    "                 pretrained: bool = False, dropout_backbone=0.1,\n",
    "                 **kwargs):\n",
    "        super(_GestureTransformer, self).__init__()\n",
    "        \n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.self_attention = EncoderSelfAttention(512, 64, 64, n_head = 8, **kwargs)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 512))\n",
    "        self.classifier = nn.Linear(512, out_planes)\n",
    "        \n",
    "        if(backbone==\"timm\"):\n",
    "            self.backbone = build_timm_backbone() \n",
    "            print(\"Finish building backbone\")\n",
    "            \n",
    "    def forward(self, x):\n",
    "        shape = x.shape\n",
    "\n",
    "        x = x.view(-1, self.in_planes, x.shape[-2], x.shape[-1])\n",
    "\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(shape[0], shape[1] // self.in_planes, -1)\n",
    "\n",
    "        x = self.self_attention(x)\n",
    "\n",
    "        x = self.pool(x).squeeze(dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "d353f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(backbone=\"timm\", in_planes = 3, out_planes=12):\n",
    "    model = _GestureTransformer(backbone = \"timm\", in_planes = in_planes, out_planes=out_planes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979671ec",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "36e4dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "class ModuleUtilizer(object):\n",
    "    \"\"\"Module utility class\n",
    "\n",
    "    Attributes:\n",
    "        configer (Configer): Configer object, contains procedure configuration.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, configer):\n",
    "        \"\"\"Class constructor for Module utility\"\"\"\n",
    "        self.configer = configer\n",
    "        self.device = self.configer.get(\"device\")\n",
    "\n",
    "        self.save_policy = self.configer.get(\"checkpoints\", \"save_policy\")\n",
    "        if self.save_policy in [\"early_stop\", \"earlystop\"]:\n",
    "            self.save = self.early_stop\n",
    "        elif self.save_policy == \"all\":\n",
    "            self.save = self.save_all\n",
    "        else:\n",
    "            self.save = self.save_best\n",
    "\n",
    "        self.best_accuracy = 0\n",
    "        self.last_improvement = 0\n",
    "\n",
    "    def update_optimizer(self, net, iters):\n",
    "        \"\"\"Load optimizer and adjust learning rate during training, if using SGD.\n",
    "\n",
    "                Args:\n",
    "                    net (torch.nn.Module): Module in use\n",
    "                    iters (int): current iteration number\n",
    "\n",
    "                Returns:\n",
    "                    optimizer (torch.optim.optimizer): PyTorch Optimizer\n",
    "                    lr (float): Learning rate for training procedure\n",
    "\n",
    "        \"\"\"\n",
    "        optim = self.configer.get('solver', 'type')\n",
    "        decay = self.configer.get('solver', 'weight_decay')\n",
    "\n",
    "        if optim == \"Adam\":\n",
    "            print(\"Using Adam.\")\n",
    "            lr = self.configer.get('solver', 'base_lr')\n",
    "            optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr,\n",
    "                                         weight_decay=decay)\n",
    "\n",
    "        elif optim == \"AdamW\":\n",
    "            lr = self.configer.get('solver', 'base_lr')\n",
    "            optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=lr,\n",
    "                                          weight_decay=decay)\n",
    "\n",
    "        elif optim == \"RMSProp\":\n",
    "            lr = self.configer.get('solver', 'base_lr')\n",
    "            optimizer = torch.optim.RMSprop(filter(lambda p: p.requires_grad, net.parameters()), lr=lr,\n",
    "                                            weight_decay=decay)\n",
    "\n",
    "        elif optim == \"SGD\":\n",
    "            print(\"Using SGD\")\n",
    "            policy = self.configer.get('solver', 'lr_policy')\n",
    "\n",
    "            if policy == 'fixed':\n",
    "                lr = self.configer.get('solver', 'base_lr')\n",
    "\n",
    "            elif policy == 'step':\n",
    "                gamma = self.configer.get('solver', 'gamma')\n",
    "                ratio = gamma ** (iters // self.configer.get('solver', 'step_size'))\n",
    "                lr = self.configer.get('solver', 'base_lr') * ratio\n",
    "\n",
    "            elif policy == 'exp':\n",
    "                lr = self.configer.get('solver', 'base_lr') * (self.configer.get('solver', 'gamma') ** iters)\n",
    "\n",
    "            elif policy == 'inv':\n",
    "                power = -self.configer.get('solver', 'power')\n",
    "                ratio = (1 + self.configer.get('solver', 'gamma') * iters) ** power\n",
    "                lr = self.configer.get('solver', 'base_lr') * ratio\n",
    "\n",
    "            elif policy == 'multistep':\n",
    "                lr = self.configer.get('solver', 'base_lr')\n",
    "                for step_value in self.configer.get('solver', 'stepvalue'):\n",
    "                    if iters >= step_value:\n",
    "                        lr *= self.configer.get('solver', 'gamma')\n",
    "                    else:\n",
    "                        break\n",
    "            else:\n",
    "                raise NotImplementedError('Policy:{} is not valid.'.format(policy))\n",
    "\n",
    "            optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr = lr,\n",
    "                                        momentum=self.configer.get('solver', 'momentum'), weight_decay=decay)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError('Optimizer: {} is not valid.'.format(optim))\n",
    "\n",
    "        return optimizer, lr\n",
    "\n",
    "    def load_net(self, net):\n",
    "        \"\"\"Loading net method. If resume is True load from provided checkpoint, if False load new DataParallel\n",
    "\n",
    "                Args:\n",
    "                    net (torch.nn.Module): Module in use\n",
    "\n",
    "                Returns:\n",
    "                    net (torch.nn.DataParallel): Loaded Network module\n",
    "                    iters (int): Loaded current iteration number, 0 if Resume is False\n",
    "                    epoch (int): Loaded current epoch number, 0 if Resume is False\n",
    "                    optimizer (torch.nn.optimizer): Loaded optimizer state, None if Resume is False\n",
    "\n",
    "        \"\"\"\n",
    "        iters = 0\n",
    "        epoch = 0\n",
    "        optimizer = None\n",
    "        if self.configer.get('resume') is not None:\n",
    "            print('Restoring checkpoint: ', self.configer.get('resume'))\n",
    "            checkpoint_dict = torch.load(self.configer.get('resume'))\n",
    "            # Remove \"module.\" from DataParallel, if present\n",
    "            checkpoint_dict['state_dict'] = {k[len('module.'):] if k.startswith('module.') else k: v for k, v in\n",
    "                                             checkpoint_dict['state_dict'].items()}\n",
    "            net.load_state_dict(checkpoint_dict['state_dict'])\n",
    "            iters = checkpoint_dict['iter'] if 'iter' in checkpoint_dict else 0\n",
    "            optimizer = checkpoint_dict['optimizer'] if 'optimizer' in checkpoint_dict else None\n",
    "            epoch = checkpoint_dict['epoch'] if 'epoch' in checkpoint_dict else None\n",
    "        net = nn.DataParallel(net, device_ids=self.configer.get('gpu')).to(self.device)\n",
    "        return net, iters, epoch, optimizer\n",
    "\n",
    "    def _save_net(self, net, optimizer, iters, epoch, all=False):\n",
    "        \"\"\"Saving net state method.\n",
    "\n",
    "                Args:\n",
    "                    net (torch.nn.Module): Module in use\n",
    "                    optimizer (torch.nn.optimizer): Optimizer state to save\n",
    "                    iters (int): Current iteration number to save\n",
    "                    epoch (int): Current epoch number to save\n",
    "\n",
    "        \"\"\"\n",
    "        state = {\n",
    "            'iter': iters,\n",
    "            'epoch': epoch,\n",
    "            'state_dict': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "        checkpoints_dir = str(Path(self.configer.get('checkpoints', 'save_dir')) / self.configer.get(\"dataset\"))\n",
    "        if not os.path.exists(checkpoints_dir):\n",
    "            os.makedirs(checkpoints_dir)\n",
    "        if all:\n",
    "            latest_name = '{}_{}.pth'.format(self.configer.get('checkpoints', 'save_name'), epoch)\n",
    "        else:\n",
    "            latest_name = 'best_{}.pth'.format(self.configer.get('checkpoints', 'save_name'))\n",
    "        torch.save(state, os.path.join(checkpoints_dir, latest_name))\n",
    "\n",
    "    def save_all(self, accuracy, net, optimizer, iters, epoch):\n",
    "        self._save_net(net, optimizer, iters, epoch, all=True)\n",
    "        return accuracy\n",
    "\n",
    "    def save_best(self, accuracy, net, optimizer, iters, epoch):\n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = accuracy\n",
    "            self._save_net(net, optimizer, iters, epoch)\n",
    "            return self.best_accuracy\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def early_stop(self, accuracy, net, optimizer, iters, epoch):\n",
    "        ret = self.save_best(accuracy, net, optimizer, iters, epoch)\n",
    "        if ret > 0:\n",
    "            self.last_improvement = 0\n",
    "        else:\n",
    "            self.last_improvement += 1\n",
    "        if self.last_improvement >= self.configer.get(\"checkpoints\", \"early_stop\"):\n",
    "            return -1\n",
    "        else:\n",
    "            return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "f3d70b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metrics(self, split: str, loss, bs, accuracy=None):\n",
    "    self.losses[split].update(loss, bs)\n",
    "    if accuracy is not None:\n",
    "        self.accuracy[split].update(accuracy, bs)\n",
    "    if split == \"train\" and self.iters % self.save_iters == 0:\n",
    "        self.tbx_summary.add_scalar('{}_loss'.format(split), self.losses[split].avg, self.iters)\n",
    "        self.tbx_summary.add_scalar('{}_accuracy'.format(split), self.accuracy[split].avg, self.iters)\n",
    "        self.losses[split].reset()\n",
    "        self.accuracy[split].reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "f42fdf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def device(self):\n",
    "    return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "628d2090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs=10):\n",
    "    \"\"\"Simple training script\"\"\"\n",
    "    # Setup device\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\n=== Loading Data ===\")\n",
    "    train_loader, test_loader, val_loader = data_loader()\n",
    "    \n",
    "    # Build model\n",
    "    print(\"\\n=== Building Model ===\")\n",
    "    model = build_model(backbone=\"timm\", in_planes=3, out_planes=12)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Setup training\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[50, 75], gamma=0.1)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\n=== Starting Training ===\")\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\n--- Epoch {epoch}/{num_epochs} ---\")\n",
    "        \n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Train\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).squeeze(1)\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Metrics\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        train_loss = train_loss / train_total\n",
    "        train_acc = train_correct / train_total\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        \n",
    "        # VALIDATION\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Val\"):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).squeeze(1)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                predicted = torch.argmax(outputs, dim=1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        val_loss = val_loss / val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"✓ Saved best model!\")\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "    \n",
    "    print(f\"\\n=== Training Complete ===\")\n",
    "    print(f\"Best Val Acc: {best_val_acc:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f1bf2f",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "c67a566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "aff45d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "=== Loading Data ===\n",
      "Loading Briareo TRAIN dataset... done.\n",
      "Loading Briareo TEST dataset... done.\n",
      "Loading Briareo VAL dataset... done.\n",
      "\n",
      "=== Building Model ===\n",
      "build_model args\n",
      "Creating model: resnet18.a1_in1k\n",
      "Finish building backbone\n",
      "\n",
      "=== Starting Training ===\n",
      "\n",
      "--- Epoch 1/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   9%|▉         | 87/936 [02:42<26:29,  1.87s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[550], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[549], line 2\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[548], line 34\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(num_epochs)\u001b[0m\n\u001b[0;32m     31\u001b[0m train_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     32\u001b[0m train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     35\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     36\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\miniconda3\\envs\\transformer\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\miniconda3\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\USER\\miniconda3\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\USER\\miniconda3\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\USER\\miniconda3\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[539], line 61\u001b[0m, in \u001b[0;36mBriareo.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     59\u001b[0m clip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[1;32m---> 61\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMREAD_COLOR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img, (\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbb39a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n",
      "(12, 0)\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)  # PyTorch's CUDA version\n",
    "print(torch.cuda.get_device_capability())  # Your GPU's compute capability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
