{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0d16ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything that is necessary\n",
    "import argparse\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e617de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch;\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40eaa033",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Briareo(Dataset):\n",
    "    \"\"\"Briareo Dataset class\"\"\"\n",
    "    def __init__(self, configer, path, split=\"train\", data_type='depth', transforms=None, n_frames=30, optical_flow=False):\n",
    "        \"\"\"Constructor method for Briareo Dataset class\n",
    "\n",
    "        Args:\n",
    "            configer (Configer): Configer object for current procedure phase (train, test, val)\n",
    "            split (str, optional): Current procedure phase (train, test, val)\n",
    "            data_type (str, optional): Input data type (depth, rgb, normals, ir)\n",
    "            transform (Object, optional): Data augmentation transformation for every data\n",
    "            n_frames (int, optional): Number of frames selected for every input clip\n",
    "            optical_flow (bool, optional): Flag to choose if calculate optical flow or not\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset_path = Path(path)\n",
    "        self.split = split\n",
    "        self.data_type = data_type\n",
    "        self.optical_flow = optical_flow\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.n_frames = n_frames + 1\n",
    "\n",
    "        print(\"Loading Briareo {} dataset...\".format(split.upper()), end=\" \")\n",
    "        data = np.load(self.dataset_path / \"splits\" / (self.split if self.split != \"val\" else \"train\") /\n",
    "                                    \"{}_{}.npz\".format(data_type, self.split), allow_pickle=True)['arr_0']\n",
    "\n",
    "        # Prepare clip for the selected number of frames n_frame\n",
    "        fixed_data = list()\n",
    "        for i, record in enumerate(data):\n",
    "            paths = record['data']\n",
    "\n",
    "            center_of_list = math.floor(len(paths) / 2)\n",
    "            crop_limit = math.floor(self.n_frames / 2)\n",
    "\n",
    "            start = center_of_list - crop_limit\n",
    "            end = center_of_list + crop_limit\n",
    "            paths_cropped = paths[start: end + 1 if self.n_frames % 2 == 1 else end]\n",
    "            if self.data_type == 'leapmotion':\n",
    "                valid = np.array(record['valid'][start: end + 1 if self.n_frames % 2 == 1 else end])\n",
    "                if valid.sum() == len(valid):\n",
    "                    data[i]['data'] = paths_cropped\n",
    "                    fixed_data.append(data[i])\n",
    "            else:\n",
    "                data[i]['data'] = paths_cropped\n",
    "                fixed_data.append(data[i])\n",
    "\n",
    "        self.data = np.array(fixed_data)\n",
    "        print(\"done.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        paths = self.data[idx]['data']\n",
    "        label = self.data[idx]['label']\n",
    "\n",
    "        clip = list()\n",
    "        for p in paths:\n",
    "            img = cv2.imread(str(self.dataset_path / p), cv2.IMREAD_COLOR)\n",
    "            img = cv2.resize(img, (224, 224))\n",
    "            if self.data_type != \"rgb\":\n",
    "                img = np.expand_dims(img, axis=2)\n",
    "            clip.append(img)\n",
    "\n",
    "        clip = np.array(clip).transpose(1, 2, 3, 0)\n",
    "\n",
    "        # if self.data_type in [\"normal\", \"normals\"]:\n",
    "        #     clip = normals_multi(clip)\n",
    "        # else:\n",
    "        #     if self.optical_flow:\n",
    "        #         clip = dense_flow(clip, self.data_type == \"rgb\")\n",
    "        #     clip = normalize(clip)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            aug_det = self.transforms.to_deterministic()\n",
    "            clip = np.array([aug_det.augment_image(clip[..., i]) for i in range(clip.shape[-1])]).transpose(1, 2, 3, 0)\n",
    "\n",
    "        clip = torch.from_numpy(clip.reshape(clip.shape[0], clip.shape[1], -1).transpose(2, 0, 1))\n",
    "        label = torch.LongTensor(np.asarray([label]))\n",
    "        return clip.float(), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c67a566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Hello\")\n",
    "    dataset_path = \"D:\\School\\Lab\\Compact-Gesture-Transformer-Code\\Briareo_rgb\"\n",
    "    train_loader = DataLoader(Briareo(configer=None, path=dataset_path, data_type=\"rgb\", split=\"train\"))\n",
    "    test_loader = DataLoader(Briareo(configer=None, path=dataset_path, data_type=\"rgb\", split=\"test\"))\n",
    "    val_loader = DataLoader(Briareo(configer=None, path=dataset_path, data_type=\"rgb\", split=\"val\"))\n",
    "\n",
    "    print(train_loader.dataset[0][0].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aff45d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Loading Briareo TRAIN dataset... done.\n",
      "Loading Briareo TEST dataset... done.\n",
      "Loading Briareo VAL dataset... done.\n",
      "torch.Size([93, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
